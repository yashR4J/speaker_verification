{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Speaker Verification - A Study in Deep Neural Networks and Transfer Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract ###\n",
    "*\"Identity theft is not a joke\" - Dwight Shrute, The Office*\n",
    "\n",
    "As the world swiftly shifts towards a technological landscape the need to protect our online identity becomes imperative. Engineers have tackled this challenge through facial and fingerprint detection, although the ability to authenticate a claimed identity through analysing a spoken sample of their voice will completely transform this space. Further, considering the major advancements in virtual reality, the ability to verify one's voice and in extension recognise their speech will be mainstream in all virtual realiy environments. Our team thus sought to extend our understanding of deep learning and neural style transfer through exploring past state-of-the-art TDNN's models and how they compare with the more accurate ECAPA-TDNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction ###\n",
    "Till recently, x-vectors have provided state-of-the-art solutions for speaker verification tasks. Usually, after convergence, speaker embeddings can be extracted from the penultimate layer to characterise a speaker in a recording. Speaker verification can thus be accomplished by comparing two embeddings with a simple cosine distance measurement. Our model expands on this through including enhancements to the TDNN architecture and statistics pooling layer. [add more]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "\n",
    "To activate a virtual environment, run\n",
    "\n",
    "```shell\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "To install the required python packages, run\n",
    "\n",
    "```shell\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "The DNN is trained to classify speakers using a training set of speech recorded from a large number of training speakers. To leverage feature representations from the pretrained model on the large dataset, speech recorded from each set of enrollment speakers is passed as input to the trained DNN. This enables the computations of deeper hidden features for each speaker in the enrollment set, which are then averaged to generate a compact deep embedding associated with that speaker.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "#### VoxCeleb\n",
    "VoxCeleb1 is a large scale audio-visual dataset for speaker identification with 150,000 samples from over 1,251 speakers. It consists of short clips of human speech, extracted from interview videos uploaded to YouTube. VoxCeleb2 is essentially the same thing but on a much larger scale. It contains over 1 million utterances from 7000 different speakers totalling over 2000 hours of both audio and video. Each segment is at least 3 seconds long and is captured 'in the wild' with background chatter, laughter and overlapping speech. The 7000 speakers span a wide range of different ethnicities, accents, professions and ages. ([SOURCE](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/))\n",
    "\n",
    "Our model is trained on audio files collected from both the VoxCeleb1 and VoxCeleb2 datasets and it achieves an accuracy of approximately 98-99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Corruption\n",
    "In realistic speech processing applications, the signal recorded by the microphone is corrupted by noise and reverberation. This is particularly harmful in distant-talking (far-field) scenarios, where the speaker and the reference microphone are distant (think about popular devices such as Google Home, Amazon Echo, Kinect, and similar devices).\n",
    "\n",
    "A common practice in neural speech processing is to start from clean speech recordings and artificially turn them into noisy ones. This process is called environmental corruption (sometimes also referred to as speech contamination). An advantage of this is that the audio can be corrupted in many different ways which increases the size of the test set. Some of those ways include Additive Noise and Reverberation.\n",
    "\n",
    "__Additive Noise__\n",
    "\n",
    "Samples from a data collection are added to the clean noise signals with a random Signal-to-Noise ratio. The amount of noise can be tuned to adjust the sampling range.\n",
    "\n",
    "__Reverberation__\n",
    "\n",
    "When speaking into a room, our speech signal is reflected multi-times by the walls, floor, ceiling, and by the objects within the acoustic environment. Consequently, the final signal recorded by a distant microphone will contain multiple delayed replicas of the original signal. All these replicas interfere with each other and significantly affect the intelligibility of the speech signal. \n",
    "\n",
    "Such a multi-path propagation is called reverberation. Within a given room enclosure, the reverberation between a source and a receiver is modeled by an impulse response. The reverberation is added by performing a convolution between a clean signal and an impulse response.\n",
    "\n",
    "__Environmental Corruption Lobe__\n",
    "\n",
    "Noise and reverberation are often combined and activated with a certain probability. The corruption operations are performed in the right order. For instance, we first introduce reverberation, and only later noise is added. We use an open-source dataset of impulse responses and noise sequences called open-rir and perform environmental corruption by sampling from it.\n",
    "\n",
    "If we call the corruption function another time, the signal is contaminated in a different way. This allows us to implement an on-the-fly speech contamination and apply different distortions to each different input. Environmental corruption is not computationally demanding and does not slow down the training loop even when doing it on-the-fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Data Augmentation\n",
    "Another way we pre-process the data is through _speech augmentation_ will also increases the size of our test data. The idea is to artificially corrupt the original speech signals to give the network the _illusion_ that we are processing a new signal. This acts as a powerful _regularizer_, that normally helps neural networks improving generalization and thus achieve better performance on test data. The augmentation techniques we use are Speed Perturbation, Time Dropout, Frequency Dropout and Clipping.\n",
    "\n",
    "__Speed Perturbation__\n",
    "\n",
    "With Speed perturbation, we resample the audio signal to a sampling rate that is a bit different from the original one. With this simple trick we can synthesize a speech signal that sounds a bit \"faster\" or \"slower\" than the original one. Note that not only the speaking rate is affected, but also the speaker characteristics such as pitch and formants.\n",
    "\n",
    "__Time Dropout__\n",
    "\n",
    "This replaces some random chunks of the original waveform with zeros. The intuition is that the neural network should provide good performance even when some piece of the signal is missing. Conceptually, this similar to dropout. The difference is that this is applied to the input waveform only. The other difference is that we drop consecutive samples rather than randomly selected elements like in dropout.\n",
    "\n",
    "__Frequency Dropout__\n",
    "\n",
    "Frequency dropout, instead of adding zeros in the time domain, it adds zeros in the frequency domain. This can be achieved by filtering the original signal with band-stop filters randomly selected. Similarly to drop chunk, the intuition is that the neural network should work well even when some frequency channels are missing.\n",
    "\n",
    "__Clipping__\n",
    "\n",
    "Another way to remove some piece of information from a speech signal is to add clipping. It a form of non-linear distortions that clamps the max absolute amplitude of the signal (thus adding a saturation effect). In the frequency domain, clipping adds harmonics in the higher part of the spectrum.\n",
    "\n",
    "__Data Augmentation Lobe__\n",
    "\n",
    "Similar to the environmental corruption lobe, the various data augmentation techniques are also applied and activated with a certain probability and this can similarly be adjusted on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture\n",
    "\n",
    "- Network Architecture (ECAPA-TDNN architecture) (See [here](https://arxiv.org/pdf/2005.07143.pdf))\n",
    "    - Channel- and context-dependent attention mechanism\n",
    "    - Multi-layer Feature Aggregation (MFA)\n",
    "    - AAMsoftmax loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/model.jpg\" style=\"width: 250px;\"/></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Block diagram of the ECAPA-TDNN model</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Used: ECAPA-TDNN ###\n",
    "#### Improvement 1: Statistical Pooling [channel-dependent attentive statistics pooling] ####\n",
    "Neural networks are known to learn hierarchical structures with each layer operating on a different level of complexity. In the ECAPA-TDNN model, features are aggregated and propagated at different hierarchical levels to produce better results. The statistics pooling module is improved with channel-dependent frame attention, enabling the network to focus on different subsets of frames in each channel statistics estimation. The frames that it does focus on depends on which frames it deems important, which is achieved through the following attention mechanism:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/attentionMechanism.jpg\" style=\"width: 250px;\"/></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Add description</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scalar score is then normalised [see Normalisation] over all frames by applying the softmax function channel-wise accross time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/softmax.png\" style=\"width: 250px;\"/></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Add description</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted mean vector and channel component are then constructed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/channel.png\" style=\"width: 250px;\"/></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Add description</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer Feature Aggregation (MFA) ####\n",
    "In the original x-vector system that our model is based off, only the final frame-layer is used for calculating the pooled statistics, although recent evidence shows that the more shallow and extensive feature maps often contribute to the most robust speaker emvbeddings. Hence we calculate the pooled statistics for every frame (Multi-layer Feature Aggregation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Speech Diarisation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import speechbrain as sb\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from user_data_prepare import prepare_user_data\n",
    "from speaker_verif.custom_train import SpkIdBrain, dataio_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How the model works ####\n",
    "Initially the model searches for all the .wav files in the specified data folder and randomly splits them into 80% for training, 10% for validation and 10% for testing purposes. The model then begins to train the network on the 80% of training data it receives through applying data preprocesing and augmentation. A Voice Activity Detection (VAD) preprocesisng step is used to detect irrelevant non-speech frames, and the augmentation is run through varying the speeds of the spoken sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import speechbrain as sb\n",
    "\n",
    "def prepare_features(self, wavs, stage):\n",
    "\n",
    "  wavs, lens = wavs\n",
    "  \n",
    "  if stage == sb.Stage.TRAIN:\n",
    "      if hasattr(self.modules, \"env_corrupt\"):\n",
    "          wavs_noise = self.modules.env_corrupt(wavs, lens)\n",
    "          wavs = torch.cat([wavs, wavs_noise], dim=0)\n",
    "          lens = torch.cat([lens, lens])\n",
    "\n",
    "      if hasattr(self.hparams, \"augmentation\"):\n",
    "          wavs = self.hparams.augmentation(wavs, lens)\n",
    "\n",
    "  # Feature extraction and normalization\n",
    "  feats = self.modules.compute_features(wavs)\n",
    "  feats = self.modules.mean_var_norm(feats, lens)\n",
    "\n",
    "  return feats, lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# pretrain folders:\n",
    "pretrained_path: speechbrain/spkrec-ecapa-voxceleb\n",
    "\n",
    "# Training Parameters\n",
    "lr: 0.001\n",
    "lr_final: 0.0001\n",
    "sample_rate: 16000\n",
    "number_of_epochs: 35\n",
    "batch_size: 32\n",
    "\n",
    "# Feature parameters\n",
    "n_mels: 80\n",
    "left_frames: 0\n",
    "right_frames: 0\n",
    "deltas: False\n",
    "\n",
    "out_n_neurons: 50 # maximum number of speakers\n",
    "emb_dim: 512 # dimensionality of the embeddings\n",
    "dataloader_options:\n",
    "    batch_size: !ref <batch_size>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Environmental Corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Added noise and reverb come from OpenRIR dataset, automatically\n",
    "# downloaded and prepared with this Environmental Corruption class.\n",
    "env_corrupt: !new:speechbrain.lobes.augment.EnvCorrupt\n",
    "    openrir_folder: !ref <data_folder>\n",
    "    babble_prob: 0.0\n",
    "    reverb_prob: 0.0\n",
    "    noise_prob: 1.0\n",
    "    noise_snr_low: 0\n",
    "    noise_snr_high: 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Adds speech change + time and frequency dropouts (time-domain implementation)\n",
    "# # A small speed change help to improve the performance of speaker-id as well.\n",
    "augmentation: !new:speechbrain.lobes.augment.TimeDomainSpecAugment\n",
    "    sample_rate: !ref <sample_rate>\n",
    "    speeds: [90, 95, 100, 105, 110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Mean and std normalization of the input features\n",
    "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
    "    norm_type: sentence\n",
    "    std_norm: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is preprocessed and augmentation is applied, the model to learn the new speaker, adjusting the weights and biases respectively. For this, we apply 1024 channels in the convolutional frame layers. The dimension of the bottleneck in the attention module is set to 128, and the number of nodes in the final fully connected layer is 192."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "compute_features: !new:speechbrain.lobes.features.Fbank\n",
    "    n_mels: !ref <n_mels>\n",
    "    left_frames: !ref <left_frames>\n",
    "    right_frames: !ref <right_frames>\n",
    "    deltas: !ref <deltas>\n",
    "\n",
    "embedding_model: !new:speechbrain.lobes.models.ECAPA_TDNN.ECAPA_TDNN\n",
    "    input_size: !ref <n_mels>\n",
    "    channels: [1024, 1024, 1024, 1024, 3072]\n",
    "    kernel_sizes: [5, 3, 3, 3, 1]\n",
    "    dilations: [1, 2, 3, 4, 1]\n",
    "    attention_channels: 128\n",
    "    lin_neurons: 192\n",
    "\n",
    "classifier: !new:speechbrain.lobes.models.ECAPA_TDNN.Classifier\n",
    "    input_size: 192\n",
    "    out_neurons: !ref <out_n_neurons>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speechbrain: A Pytorch-based Speech Toolkit\n",
    "\n",
    "Add some text here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Path to model hyperparameters file\n",
    "hparams_file = \"speaker_verif/custom_train.yaml\"\n",
    "\n",
    "# Initialize ddp (useful only for multi-GPU DDP training).\n",
    "sb.utils.distributed.ddp_init_group(run_opts)\n",
    "\n",
    "# Load hyperparameters file with command-line overrides.\n",
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "# Create experiment directory\n",
    "sb.create_experiment_directory(\n",
    "    experiment_directory=hparams[\"output_folder\"],\n",
    "    hyperparams_to_save=hparams_file,\n",
    "    overrides=overrides,\n",
    ")\n",
    "\n",
    "# Data preparation, to be run on only one process.\n",
    "sb.utils.distributed.run_on_main(\n",
    "    prepare_user_data,\n",
    "    kwargs={\n",
    "        \"data_folder\": hparams[\"data_folder\"],\n",
    "        \"save_json_train\": hparams[\"train_annotation\"],\n",
    "        \"save_json_valid\": hparams[\"valid_annotation\"],\n",
    "        \"save_json_test\": hparams[\"test_annotation\"],\n",
    "        \"split_ratio\": [80, 10, 10],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Load the pretrained model\n",
    "if \"pretrainer\" in hparams:\n",
    "    hparams[\"pretrainer\"].collect_files()\n",
    "    hparams[\"pretrainer\"].load_collected(device=run_opts[\"device\"])\n",
    "else:\n",
    "    print(\"No pretrained model found, training from scratch.\")\n",
    "    \n",
    "# Create dataset objects \"train\", \"valid\", and \"test\".\n",
    "datasets = dataio_prep(hparams)\n",
    "\n",
    "# Initialize the Brain object to prepare for mask training.\n",
    "spk_id_brain = SpkIdBrain(\n",
    "    modules=hparams[\"modules\"],\n",
    "    opt_class=hparams[\"opt_class\"],\n",
    "    hparams=hparams,\n",
    "    run_opts=run_opts,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    ")   \n",
    "\n",
    "# The `fit()` method iterates the training loop, calling the methods\n",
    "# necessary to update the parameters of the model. Since all objects\n",
    "# with changing state are managed by the Checkpointer, training can be\n",
    "# stopped at any point, and will be resumed on next call.\n",
    "spk_id_brain.fit(\n",
    "    epoch_counter=spk_id_brain.hparams.epoch_counter,\n",
    "    train_set=datasets[\"train\"],\n",
    "    valid_set=datasets[\"valid\"],\n",
    "    train_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
    ")\n",
    "\n",
    "# Load the best checkpoint for evaluation\n",
    "test_stats = spk_id_brain.evaluate(\n",
    "    test_set=datasets[\"test\"],\n",
    "    min_key=\"error\",\n",
    "    test_loader_kwargs=hparams[\"dataloader_options\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification through Inference\n",
    "\n",
    "To verify the identity of an unknown speaker, a test utterance of the unknown speaker is passed as input to the trained DNN. A compact deep embedding associated with the unknown speaker is generated and compared with the compact deep embeddings associated with each of the enrollment speakers through calculation of Cosine Distance Similarity. (Talk about Cosine Distance - include brief background?). The distance between the compared compact deep embeddings corresponds to the likelihood that the unknown speaker belongs to the set of enrolled speakers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Imports for inference\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from random import shuffle\n",
    "from torch.nn import CosineSimilarity \n",
    "from torchaudio import load as load_signal\n",
    "from speechbrain.pretrained import EncoderClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable easy accessibility to the most recently trained model, we first move it to the \"content/best_model\" path along with associated hyperparameters and class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "src_path = \"results/speaker_id/1986/save/\"  # Path to trained network checkpoints\n",
    "dest_path = \"content/best_model/\"           # Path to store most recently trained model information \n",
    "\n",
    "if os.path.exists(dest_path):\n",
    "    shutil.rmtree(dest_path)\n",
    "\n",
    "os.mkdir(dest_path)\n",
    "shutil.copy2(\"./hparams_inference.yaml\", dest_path)\n",
    "shutil.copy2(src_path + \"label_encoder.txt\", dest_path)\n",
    "ckpt_files = glob.glob(src_path + \"CKPT*\")\n",
    "if not ckpt_files:\n",
    "    print(\"No trained checkpoints\")\n",
    "    exit(1)\n",
    "latest_ckpt_path = max(ckpt_files, key=os.path.getctime)\n",
    "for file in glob.glob(latest_ckpt_path + \"/*\"):\n",
    "    shutil.copy2(file, dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to begin inference, we identify the path to the recorded test signal and the *unique* user id that the test signal should be tested against. << Briefly explain EncoderClassifier class. >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Build Classifier\n",
    "classifier = EncoderClassifier.from_hparams(source=\"content/best_model\",  hparams_file='hparams_inference.yaml', savedir=\"content/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPLAIN COSINE SIMILARITY "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/cos_sim.png\" style=\"width: 500px;\"/></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Calculation of Cosine Similarity</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "similarity = CosineSimilarity(dim=-1, eps=1e-8) # dim=-1 refers to the last dimension (i.e. the embedding dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The verification process is divided into two sections: extracting vector embeddings for each voice signal and calculating its similarity to one of the recorded samples from the enrolled speaker. To allow for a better measure of speaker validation, we test the test signal against 5 randomly selected voice samples from the enrolled speaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def extract_audio_embeddings(model, wav_audio_file_path: str) -> tuple:\n",
    "    \"\"\"Feature extractor that embeds audio into a vector.\"\"\"\n",
    "    signal, _ = load_signal(wav_audio_file_path)  # Reformat audio signal into a tensor\n",
    "    embeddings = model.encode_batch(\n",
    "        signal\n",
    "    )  # Pass tensor through pretrained neural net and extract representation\n",
    "    return embeddings\n",
    "\n",
    "def verify(s1, s2):\n",
    "    global similarity\n",
    "    THRESHOLD = 0.25\n",
    "    score = similarity(s1, s2) # resulting tensor has scores = embedding dimensionality \n",
    "    for s in score: \n",
    "        if s > THRESHOLD: return True\n",
    "    return False\n",
    "\n",
    "test_emb = extract_audio_embeddings(classifier, test_signal_path)\n",
    "\n",
    "spk_samples = glob.glob(f\"data/user_data/raw/{spk_id}/*/*.wav\")\n",
    "shuffle(spk_samples)\n",
    "for sample_path in spk_samples[:5]: # test on up to 5 random samples\n",
    "    print(f\"Testing sample against {sample_path}\")\n",
    "    sample_emb = extract_audio_embeddings(classifier, sample_path)\n",
    "    if verify(test_emb, sample_emb):\n",
    "        print(\"User Verified\")\n",
    "        exit(0)\n",
    "\n",
    "print(\"Suspicious User - Access Denied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/ecapa-tdnn-7205.png\" style=\"width: 400px;\"/></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Training Loss and Validation Loss for Pretrained Model</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### System Performance\n",
    "\n",
    "Some text here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Strengths\n",
    "\n",
    "Text here\n",
    "\n",
    "##### Weaknesses and Limitations\n",
    "\n",
    "<ins>Cosine Threshold Determination</ins> \n",
    "\n",
    "text here\n",
    "\n",
    "<ins>Biases</ins> \n",
    "\n",
    "- Baseline pretrained model - 61% male, 29% female (skewed towards males) (Representation bias)\n",
    "- Measurement bias\n",
    "- Evaluation bias (in pretrained model and our model --> determination of output neurons is based on maximum allowed users in app)\n",
    "- Preprocessing (environmental noise is not removed to create clean sample in our model) -- improve in future work\n",
    "\n",
    "[Article](https://arxiv.org/pdf/2201.09486.pdf)\n",
    "\n",
    "##### Possible Future Work\n",
    "\n",
    "Text here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Features to Discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Network Architecture (ECAPA-TDNN architecture) (See [here](https://arxiv.org/pdf/2005.07143.pdf)) (Ahmet)\n",
    "    - Channel- and context-dependent attention mechanism\n",
    "    - Multi-layer Feature Aggregation (MFA)\n",
    "    - AAMsoftmax loss\n",
    "- Connectionist temporal classification loss (CTC loss)\n",
    "- VAD\n",
    "- Statistical pooling (Ahmet)\n",
    "- Data Augmentation (Adding time/frequency dropouts, speed change, environmental corruption, noise addition) (Armaan)\n",
    "- Dropout\n",
    "- Normalisation\n",
    "- Linear Learning Rate Decay and Adam Optimiser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion ###\n",
    "While speaker verification on its own cannot guarantee security, it will add strength and friction to our online identities and reduce the likelihood of incorrect authentication. A future experiment of ours is to use a similar framework for speech recognition, and eventually using neural networks to harness the ability of speaking in the voice of another person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citation\n",
    "### References\n",
    "\n",
    "##### Datasets\n",
    "\n",
    "@InProceedings{Nagrani17,\n",
    "  author       = \"Nagrani, A. and Chung, J.~S. and Zisserman, A.\",\n",
    "  title        = \"VoxCeleb: a large-scale speaker identification dataset\",\n",
    "  booktitle    = \"INTERSPEECH\",\n",
    "  year         = \"2017\",\n",
    "}\n",
    "\n",
    "@InProceedings{Nagrani17,\n",
    "  author       = \"Chung, J.~S. and Nagrani, A. and Zisserman, A.\",\n",
    "  title        = \"VoxCeleb2: Deep Speaker Recognition\",\n",
    "  booktitle    = \"INTERSPEECH\",\n",
    "  year         = \"2018\",\n",
    "}\n",
    "\n",
    "##### Other\n",
    "\n",
    "@misc{speechbrain,\n",
    "  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n",
    "  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n",
    "  year={2021},\n",
    "  eprint={2106.04624},\n",
    "  archivePrefix={arXiv},\n",
    "  primaryClass={eess.AS},\n",
    "  note={arXiv:2106.04624}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd179667f0ddb41f867bd7bfe774b77331caa106e6d217b9af70320afb96d8b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
