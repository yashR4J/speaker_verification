{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Speaker Verification - A Study in Deep Neural Networks and Transfer Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract ###\n",
    "*\"Identity theft is not a joke\" - Dwight Shrute, The Office*\n",
    "\n",
    "As the world swiftly shifts towards a technological landscape the need to protect our online identity becomes imperative. Engineers have tackled this challenge through facial and fingerprint detection, although the ability to authenticate a claimed identity through analysing a spoken sample of their voice will completely transform this space. Further, considering the major advancements in virtual reality, the ability to verify one's voice and in extension recognise their speech will be mainstream in all virtual realiy environments. Our team thus sought to extend our understanding of deep learning and neural style transfer through exploring past state-of-the-art TDNN's models and how they compare with the more accurate ECAPA-TDNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction ###\n",
    "Till recently, x-vectors have provided state-of-the-art solutions for speaker verification tasks. Usually, after convergence, speaker embeddings can be extracted from the penultimate layer to characterise a speaker in a recording. Speaker verification can thus be accomplished by comparing two embeddings with a simple cosine distance measurement. Our model expands on this through including enhancements to the TDNN architecture and statistics pooling layer. [add more]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "\n",
    "To activate a virtual environment, run\n",
    "\n",
    "```shell\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "To install the required python packages, run\n",
    "\n",
    "```shell\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "The DNN is trained to classify speakers using a training set of speech recorded from a large number of training speakers. (Talk about Vox-Celeb dataset). To leverage feature representations from the pretrained model on the large dataset, speech recorded from each set of enrollment speakers is passed as input to the trained DNN. This enables the computations of deeper hidden features for each speaker in the enrollment set, which are then averaged to generate a compact deep embedding associated with that speaker.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Our pretrained model is trained on audio files collected from the VoxCeleb1 + VoxCeleb2 dataset, consisting of speech samples from over 7000 different speakers of a wide range of ethnicities, accents, professions and ages ([SOURCE](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/)). Achieves an accuracy of approximately 98-99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture\n",
    "\n",
    "- Network Architecture (ECAPA-TDNN architecture) (See [here](https://arxiv.org/pdf/2005.07143.pdf))\n",
    "    - Channel- and context-dependent attention mechanism\n",
    "    - Multi-layer Feature Aggregation (MFA)\n",
    "    - AAMsoftmax loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/model.jpg\" style=\"width: 250px;\"/></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Block diagram of the ECAPA-TDNN model</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Used: ECAPA-TDNN ###\n",
    "#### Improvement 1: Statistical Pooling [channel-dependent attentive statistics pooling] ####\n",
    "Neural networks are known to learn heirarchical structures with each layer operating on a different level of complexity. In the ECAPA-TDNN model, features are aggregated and propagated at different hierarchical levels to produce better results. The statistics pooling module is improved with channel-dependent frame attention, enabling the network to focus on different subsets of frames in each channel statistics estimation. The frames that it does focus on depends on which frames it deems important, which is achieved through the following attention mechanism:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(os\u001b[39m.\u001b[39;49mdir)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'os' has no attribute 'dir'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# Image(\"images/attentionMechanism.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scalar score is then normalised [see Normalisation] over all frames by applying the softmax function channel-wise accross time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: 'images/softmax.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:1032\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1032\u001b[0m     b64_data \u001b[39m=\u001b[39m b2a_base64(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1033\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/formatters.py:973\u001b[0m, in \u001b[0;36mMimeBundleFormatter.__call__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    970\u001b[0m     method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n\u001b[1;32m    972\u001b[0m     \u001b[39mif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 973\u001b[0m         \u001b[39mreturn\u001b[39;00m method(include\u001b[39m=\u001b[39;49minclude, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[1;32m    974\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:1022\u001b[0m, in \u001b[0;36mImage._repr_mimebundle_\u001b[0;34m(self, include, exclude)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed:\n\u001b[1;32m   1021\u001b[0m     mimetype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mimetype\n\u001b[0;32m-> 1022\u001b[0m     data, metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_and_metadata(always_both\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1023\u001b[0m     \u001b[39mif\u001b[39;00m metadata:\n\u001b[1;32m   1024\u001b[0m         metadata \u001b[39m=\u001b[39m {mimetype: metadata}\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:1034\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     b64_data \u001b[39m=\u001b[39m b2a_base64(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1033\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1035\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m md \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1037\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: 'images/softmax.png'"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: 'images/softmax.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:1032\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1032\u001b[0m     b64_data \u001b[39m=\u001b[39m b2a_base64(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1033\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/formatters.py:343\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    341\u001b[0m     method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n\u001b[1;32m    342\u001b[0m     \u001b[39mif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m         \u001b[39mreturn\u001b[39;00m method()\n\u001b[1;32m    344\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:1054\u001b[0m, in \u001b[0;36mImage._repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_repr_png_\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1053\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_FMT_PNG:\n\u001b[0;32m-> 1054\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_and_metadata()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:1034\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     b64_data \u001b[39m=\u001b[39m b2a_base64(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1033\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1035\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m md \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1037\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: 'images/softmax.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/softmax.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted mean vector and channel component are then constructed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: 'images/channel.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:1032\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1032\u001b[0m     b64_data \u001b[39m=\u001b[39m b2a_base64(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1033\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/formatters.py:973\u001b[0m, in \u001b[0;36mMimeBundleFormatter.__call__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    970\u001b[0m     method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n\u001b[1;32m    972\u001b[0m     \u001b[39mif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 973\u001b[0m         \u001b[39mreturn\u001b[39;00m method(include\u001b[39m=\u001b[39;49minclude, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[1;32m    974\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:1022\u001b[0m, in \u001b[0;36mImage._repr_mimebundle_\u001b[0;34m(self, include, exclude)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed:\n\u001b[1;32m   1021\u001b[0m     mimetype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mimetype\n\u001b[0;32m-> 1022\u001b[0m     data, metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_and_metadata(always_both\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1023\u001b[0m     \u001b[39mif\u001b[39;00m metadata:\n\u001b[1;32m   1024\u001b[0m         metadata \u001b[39m=\u001b[39m {mimetype: metadata}\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:1034\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     b64_data \u001b[39m=\u001b[39m b2a_base64(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1033\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1035\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m md \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1037\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: 'images/channel.png'"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: 'images/channel.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:1032\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1032\u001b[0m     b64_data \u001b[39m=\u001b[39m b2a_base64(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1033\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/formatters.py:343\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    341\u001b[0m     method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n\u001b[1;32m    342\u001b[0m     \u001b[39mif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m         \u001b[39mreturn\u001b[39;00m method()\n\u001b[1;32m    344\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:1054\u001b[0m, in \u001b[0;36mImage._repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_repr_png_\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1053\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_FMT_PNG:\n\u001b[0;32m-> 1054\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_and_metadata()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:1034\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     b64_data \u001b[39m=\u001b[39m b2a_base64(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1033\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1035\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m md \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1037\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: 'images/channel.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/channel.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer Feature Aggregation (MFA) ####\n",
    "In the original x-vector system that our model is based off, only the final frame-layer is used for calculating the pooled statistics, although recent evidence shows that the more shallow and extensive feature maps often contribute to the most robust speaker emvbeddings. Hence we calculate the pooled statistics for every frame (Multi-layer Feature Aggregation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How the model works ####\n",
    "Initially the model searches for all the .wav files in the specified data folder and randomly splits them into 80% for training, 10% for validation and 10% for testing purposes. The model then begins to train the network on the 80% of training data it receives through applying data preprocesing and augmentation. A Voice Activity Detection (VAD) preprocesisng step is used to detect irrelevant non-speech frames, and the augmentation is run through varying the speeds of the spoken sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import speechbrain as sb\n",
    "\n",
    "def prepare_features(self, wavs, stage):\n",
    "\n",
    "  wavs, lens = wavs\n",
    "  \n",
    "  if stage == sb.Stage.TRAIN:\n",
    "      if hasattr(self.modules, \"env_corrupt\"):\n",
    "          wavs_noise = self.modules.env_corrupt(wavs, lens)\n",
    "          wavs = torch.cat([wavs, wavs_noise], dim=0)\n",
    "          lens = torch.cat([lens, lens])\n",
    "\n",
    "      if hasattr(self.hparams, \"augmentation\"):\n",
    "          wavs = self.hparams.augmentation(wavs, lens)\n",
    "\n",
    "  # Feature extraction and normalization\n",
    "  feats = self.modules.compute_features(wavs)\n",
    "  feats = self.modules.mean_var_norm(feats, lens)\n",
    "\n",
    "  return feats, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Adds speech change + time and frequency dropouts (time-domain implementation)\n",
    "# # A small speed change help to improve the performance of speaker-id as well.\n",
    "augmentation: !new:speechbrain.lobes.augment.TimeDomainSpecAugment\n",
    "    sample_rate: !ref <sample_rate>\n",
    "    speeds: [90, 95, 100, 105, 110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is preprocessed and augmentation is applied, the model to learn the new speaker, adjusting the weights and biases respectively. For this, we apply 1024 channels in the convolutional frame layers. The dimension of the bottleneck in the attention module is set to 128, and the number of nodes in the final fully connected layer is 192."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "embedding_model: !new:speechbrain.lobes.models.ECAPA_TDNN.ECAPA_TDNN\n",
    "    input_size: !ref <n_mels>\n",
    "    channels: [1024, 1024, 1024, 1024, 3072]\n",
    "    kernel_sizes: [5, 3, 3, 3, 1]\n",
    "    dilations: [1, 2, 3, 4, 1]\n",
    "    attention_channels: 128\n",
    "    lin_neurons: 192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Speech Diarisation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speechbrain as sb\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from user_data_prepare import prepare_user_data\n",
    "from speaker_verif.custom_train import SpkIdBrain, dataio_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# pretrain folders:\n",
    "pretrained_path: speechbrain/spkrec-ecapa-voxceleb\n",
    "\n",
    "# Training Parameters\n",
    "lr: 0.001\n",
    "lr_final: 0.0001\n",
    "sample_rate: 16000\n",
    "number_of_epochs: 35\n",
    "batch_size: 32\n",
    "\n",
    "# Feature parameters\n",
    "n_mels: 80\n",
    "left_frames: 0\n",
    "right_frames: 0\n",
    "deltas: False\n",
    "\n",
    "out_n_neurons: 50 # maximum number of speakers\n",
    "emb_dim: 512 # dimensionality of the embeddings\n",
    "dataloader_options:\n",
    "    batch_size: !ref <batch_size>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Environmental Corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Added noise and reverb come from OpenRIR dataset, automatically\n",
    "# downloaded and prepared with this Environmental Corruption class.\n",
    "env_corrupt: !new:speechbrain.lobes.augment.EnvCorrupt\n",
    "    openrir_folder: !ref <data_folder>\n",
    "    babble_prob: 0.0\n",
    "    reverb_prob: 0.0\n",
    "    noise_prob: 1.0\n",
    "    noise_snr_low: 0\n",
    "    noise_snr_high: 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Adds speech change + time and frequency dropouts (time-domain implementation)\n",
    "# # A small speed change help to improve the performance of speaker-id as well.\n",
    "augmentation: !new:speechbrain.lobes.augment.TimeDomainSpecAugment\n",
    "    sample_rate: !ref <sample_rate>\n",
    "    speeds: [90, 95, 100, 105, 110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Mean and std normalization of the input features\n",
    "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
    "    norm_type: sentence\n",
    "    std_norm: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "compute_features: !new:speechbrain.lobes.features.Fbank\n",
    "    n_mels: !ref <n_mels>\n",
    "    left_frames: !ref <left_frames>\n",
    "    right_frames: !ref <right_frames>\n",
    "    deltas: !ref <deltas>\n",
    "\n",
    "embedding_model: !new:speechbrain.lobes.models.ECAPA_TDNN.ECAPA_TDNN\n",
    "    input_size: !ref <n_mels>\n",
    "    channels: [1024, 1024, 1024, 1024, 3072]\n",
    "    kernel_sizes: [5, 3, 3, 3, 1]\n",
    "    dilations: [1, 2, 3, 4, 1]\n",
    "    attention_channels: 128\n",
    "    lin_neurons: 192\n",
    "\n",
    "classifier: !new:speechbrain.lobes.models.ECAPA_TDNN.Classifier\n",
    "    input_size: 192\n",
    "    out_neurons: !ref <out_n_neurons>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speechbrain: A Pytorch-based Speech Toolkit\n",
    "\n",
    "Add some text here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to model hyperparameters file\n",
    "hparams_file = \"speaker_verif/custom_train.yaml\"\n",
    "\n",
    "# Initialize ddp (useful only for multi-GPU DDP training).\n",
    "sb.utils.distributed.ddp_init_group(run_opts)\n",
    "\n",
    "# Load hyperparameters file with command-line overrides.\n",
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "# Create experiment directory\n",
    "sb.create_experiment_directory(\n",
    "    experiment_directory=hparams[\"output_folder\"],\n",
    "    hyperparams_to_save=hparams_file,\n",
    "    overrides=overrides,\n",
    ")\n",
    "\n",
    "# Data preparation, to be run on only one process.\n",
    "sb.utils.distributed.run_on_main(\n",
    "    prepare_user_data,\n",
    "    kwargs={\n",
    "        \"data_folder\": hparams[\"data_folder\"],\n",
    "        \"save_json_train\": hparams[\"train_annotation\"],\n",
    "        \"save_json_valid\": hparams[\"valid_annotation\"],\n",
    "        \"save_json_test\": hparams[\"test_annotation\"],\n",
    "        \"split_ratio\": [80, 10, 10],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Load the pretrained model\n",
    "if \"pretrainer\" in hparams:\n",
    "    hparams[\"pretrainer\"].collect_files()\n",
    "    hparams[\"pretrainer\"].load_collected(device=run_opts[\"device\"])\n",
    "else:\n",
    "    print(\"No pretrained model found, training from scratch.\")\n",
    "    \n",
    "# Create dataset objects \"train\", \"valid\", and \"test\".\n",
    "datasets = dataio_prep(hparams)\n",
    "\n",
    "# Initialize the Brain object to prepare for mask training.\n",
    "spk_id_brain = SpkIdBrain(\n",
    "    modules=hparams[\"modules\"],\n",
    "    opt_class=hparams[\"opt_class\"],\n",
    "    hparams=hparams,\n",
    "    run_opts=run_opts,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    ")   \n",
    "\n",
    "# The `fit()` method iterates the training loop, calling the methods\n",
    "# necessary to update the parameters of the model. Since all objects\n",
    "# with changing state are managed by the Checkpointer, training can be\n",
    "# stopped at any point, and will be resumed on next call.\n",
    "spk_id_brain.fit(\n",
    "    epoch_counter=spk_id_brain.hparams.epoch_counter,\n",
    "    train_set=datasets[\"train\"],\n",
    "    valid_set=datasets[\"valid\"],\n",
    "    train_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
    ")\n",
    "\n",
    "# Load the best checkpoint for evaluation\n",
    "test_stats = spk_id_brain.evaluate(\n",
    "    test_set=datasets[\"test\"],\n",
    "    min_key=\"error\",\n",
    "    test_loader_kwargs=hparams[\"dataloader_options\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification through Inference\n",
    "\n",
    "To verify the identity of an unknown speaker, a test utterance of the unknown speaker is passed as input to the trained DNN. A compact deep embedding associated with the unknown speaker is generated and compared with the compact deep embeddings associated with each of the enrollment speakers through calculation of Cosine Distance Similarity. (Talk about Cosine Distance - include brief background?). The distance between the compared compact deep embeddings corresponds to the likelihood that the unknown speaker belongs to the set of enrolled speakers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for inference\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from random import shuffle\n",
    "from torch.nn import CosineSimilarity \n",
    "from torchaudio import load as load_signal\n",
    "from speechbrain.pretrained import EncoderClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable easy accessibility to the most recently trained model, we first move it to the \"content/best_model\" path along with associated hyperparameters and class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = \"results/speaker_id/1986/save/\"  # Path to trained network checkpoints\n",
    "dest_path = \"content/best_model/\"           # Path to store most recently trained model information \n",
    "\n",
    "if os.path.exists(dest_path):\n",
    "    shutil.rmtree(dest_path)\n",
    "\n",
    "os.mkdir(dest_path)\n",
    "shutil.copy2(\"./hparams_inference.yaml\", dest_path)\n",
    "shutil.copy2(src_path + \"label_encoder.txt\", dest_path)\n",
    "ckpt_files = glob.glob(src_path + \"CKPT*\")\n",
    "if not ckpt_files:\n",
    "    print(\"No trained checkpoints\")\n",
    "    exit(1)\n",
    "latest_ckpt_path = max(ckpt_files, key=os.path.getctime)\n",
    "for file in glob.glob(latest_ckpt_path + \"/*\"):\n",
    "    shutil.copy2(file, dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to begin inference, we identify the path to the recorded test signal and the *unique* user id that the test signal should be tested against. << Briefly explain EncoderClassifier class. >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Classifier\n",
    "classifier = EncoderClassifier.from_hparams(source=\"content/best_model\",  hparams_file='hparams_inference.yaml', savedir=\"content/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/cos_sim.png\" style=\"width: 500px;\"/></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Calculation of Cosine Similarity</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity\n",
    "similarity = CosineSimilarity(dim=-1, eps=1e-8) # dim=-1 refers to the last dimension (i.e. the embedding dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The verification process is divided into two sections: extracting vector embeddings for each voice signal and calculating its similarity to one of the recorded samples from the enrolled speaker. To allow for a better measure of speaker validation, we test the test signal against 5 randomly selected voice samples from the enrolled speaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_embeddings(model, wav_audio_file_path: str) -> tuple:\n",
    "    \"\"\"Feature extractor that embeds audio into a vector.\"\"\"\n",
    "    signal, _ = load_signal(wav_audio_file_path)  # Reformat audio signal into a tensor\n",
    "    embeddings = model.encode_batch(\n",
    "        signal\n",
    "    )  # Pass tensor through pretrained neural net and extract representation\n",
    "    return embeddings\n",
    "\n",
    "def verify(s1, s2):\n",
    "    global similarity\n",
    "    THRESHOLD = 0.25\n",
    "    score = similarity(s1, s2) # resulting tensor has scores = embedding dimensionality \n",
    "    for s in score: \n",
    "        if s > THRESHOLD: return True\n",
    "    return False\n",
    "\n",
    "test_emb = extract_audio_embeddings(classifier, test_signal_path)\n",
    "\n",
    "spk_samples = glob.glob(f\"data/user_data/raw/{spk_id}/*/*.wav\")\n",
    "shuffle(spk_samples)\n",
    "for sample_path in spk_samples[:5]: # test on up to 5 random samples\n",
    "    print(f\"Testing sample against {sample_path}\")\n",
    "    sample_emb = extract_audio_embeddings(classifier, sample_path)\n",
    "    if verify(test_emb, sample_emb):\n",
    "        print(\"User Verified\")\n",
    "        exit(0)\n",
    "\n",
    "print(\"Suspicious User - Access Denied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Features to Discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Network Architecture (ECAPA-TDNN architecture) (See [here](https://arxiv.org/pdf/2005.07143.pdf)) (Ahmet)\n",
    "    - Channel- and context-dependent attention mechanism\n",
    "    - Multi-layer Feature Aggregation (MFA)\n",
    "    - AAMsoftmax loss\n",
    "- Connectionist temporal classification loss (CTC loss)\n",
    "- VAD\n",
    "- Statistical pooling (Ahmet)\n",
    "- Data Augmentation (Adding time/frequency dropouts, speed change, environmental corruption, noise addition) (Armaan)\n",
    "- Dropout\n",
    "- Normalisation\n",
    "- Linear Learning Rate Decay and Adam Optimiser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion ###\n",
    "While speaker verification on its own cannot guarantee security, it will add strength and friction to our online identities and reduce the likelihood of incorrect authentication. A future experiment of ours is to use a similar framework for speech recognition, and eventually using neural networks to harness the ability of speaking in the voice of another person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citation\n",
    "### References\n",
    "\n",
    "##### Datasets\n",
    "\n",
    "@InProceedings{Nagrani17,\n",
    "  author       = \"Nagrani, A. and Chung, J.~S. and Zisserman, A.\",\n",
    "  title        = \"VoxCeleb: a large-scale speaker identification dataset\",\n",
    "  booktitle    = \"INTERSPEECH\",\n",
    "  year         = \"2017\",\n",
    "}\n",
    "\n",
    "@InProceedings{Nagrani17,\n",
    "  author       = \"Chung, J.~S. and Nagrani, A. and Zisserman, A.\",\n",
    "  title        = \"VoxCeleb2: Deep Speaker Recognition\",\n",
    "  booktitle    = \"INTERSPEECH\",\n",
    "  year         = \"2018\",\n",
    "}\n",
    "\n",
    "##### Other\n",
    "\n",
    "@misc{speechbrain,\n",
    "  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n",
    "  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n",
    "  year={2021},\n",
    "  eprint={2106.04624},\n",
    "  archivePrefix={arXiv},\n",
    "  primaryClass={eess.AS},\n",
    "  note={arXiv:2106.04624}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
