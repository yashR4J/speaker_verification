{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Speaker Verification - A Study in Deep Neural Networks and Transfer Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"Identity theft is not a joke\" - Dwight Shrute, The Office*\n",
    "\n",
    "Automated speaker verification (ASV), in the modern day, is omnipresent in smart devices and in services offered by call centres. It serves as a biometric means of authenticating a claimed identity by analysing a spoken sample of their voice. \n",
    "our team sought to extend our understanding of neural style transfer by exploring how convolutional neural networks could be used to harness the ability to speak in the voice of another person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "\n",
    "To activate a virtual environment, run\n",
    "\n",
    "```shell\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "To install the required python packages, run\n",
    "\n",
    "```shell\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "The DNN is trained to classify speakers using a training set of speech recorded from a large number of training speakers. (Talk about Vox-Celeb dataset). To leverage feature representations from the pretrained model on the large dataset, speech recorded from each set of enrollment speakers is passed as input to the trained DNN. This enables the computations of deeper hidden features for each speaker in the enrollment set, which are then averaged to generate a compact deep embedding associated with that speaker.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Our pretrained model is trained on audio files collected from the VoxCeleb1 + VoxCeleb2 dataset, consisting of speech samples from over 7000 different speakers of a wide range of ethnicities, accents, professions and ages ([SOURCE](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/)). Achieves an accuracy of approximately 98-99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture\n",
    "\n",
    "- Network Architecture (ECAPA-TDNN architecture) (See [here](https://arxiv.org/pdf/2005.07143.pdf))\n",
    "    - Channel- and context-dependent attention mechanism\n",
    "    - Multi-layer Feature Aggregation (MFA)\n",
    "    - AAMsoftmax loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/model.jpg\" style=\"width: 250px;\"/></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Block diagram of the ECAPA-TDNN model</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Speech Diarisation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speechbrain as sb\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from user_data_prepare import prepare_user_data\n",
    "from speaker_verif.custom_train import SpkIdBrain, dataio_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "# pretrain folders:\n",
    "pretrained_path: speechbrain/spkrec-ecapa-voxceleb\n",
    "\n",
    "# Training Parameters\n",
    "lr: 0.001\n",
    "lr_final: 0.0001\n",
    "sample_rate: 16000\n",
    "number_of_epochs: 35\n",
    "batch_size: 32\n",
    "\n",
    "# Feature parameters\n",
    "n_mels: 80\n",
    "left_frames: 0\n",
    "right_frames: 0\n",
    "deltas: False\n",
    "\n",
    "out_n_neurons: 50 # maximum number of speakers\n",
    "emb_dim: 512 # dimensionality of the embeddings\n",
    "dataloader_options:\n",
    "    batch_size: !ref <batch_size>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Environmental Corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "# Added noise and reverb come from OpenRIR dataset, automatically\n",
    "# downloaded and prepared with this Environmental Corruption class.\n",
    "env_corrupt: !new:speechbrain.lobes.augment.EnvCorrupt\n",
    "    openrir_folder: !ref <data_folder>\n",
    "    babble_prob: 0.0\n",
    "    reverb_prob: 0.0\n",
    "    noise_prob: 1.0\n",
    "    noise_snr_low: 0\n",
    "    noise_snr_high: 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "# Adds speech change + time and frequency dropouts (time-domain implementation)\n",
    "# # A small speed change help to improve the performance of speaker-id as well.\n",
    "augmentation: !new:speechbrain.lobes.augment.TimeDomainSpecAugment\n",
    "    sample_rate: !ref <sample_rate>\n",
    "    speeds: [90, 95, 100, 105, 110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "# Mean and std normalization of the input features\n",
    "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
    "    norm_type: sentence\n",
    "    std_norm: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "compute_features: !new:speechbrain.lobes.features.Fbank\n",
    "    n_mels: !ref <n_mels>\n",
    "    left_frames: !ref <left_frames>\n",
    "    right_frames: !ref <right_frames>\n",
    "    deltas: !ref <deltas>\n",
    "\n",
    "embedding_model: !new:speechbrain.lobes.models.ECAPA_TDNN.ECAPA_TDNN\n",
    "    input_size: !ref <n_mels>\n",
    "    channels: [1024, 1024, 1024, 1024, 3072]\n",
    "    kernel_sizes: [5, 3, 3, 3, 1]\n",
    "    dilations: [1, 2, 3, 4, 1]\n",
    "    attention_channels: 128\n",
    "    lin_neurons: 192\n",
    "\n",
    "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
    "    input_shape: [null, null, 192]\n",
    "    activation: !name:torch.nn.LeakyReLU\n",
    "    lin_blocks: 1\n",
    "    lin_neurons: 512\n",
    "    out_neurons: !ref <out_n_neurons>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speechbrain: A Pytorch-based Speech Toolkit\n",
    "\n",
    "Add some text here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to model hyperparameters file\n",
    "hparams_file = \"speaker_verif/custom_train.yaml\"\n",
    "\n",
    "# Initialize ddp (useful only for multi-GPU DDP training).\n",
    "sb.utils.distributed.ddp_init_group(run_opts)\n",
    "\n",
    "# Load hyperparameters file with command-line overrides.\n",
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "# Create experiment directory\n",
    "sb.create_experiment_directory(\n",
    "    experiment_directory=hparams[\"output_folder\"],\n",
    "    hyperparams_to_save=hparams_file,\n",
    "    overrides=overrides,\n",
    ")\n",
    "\n",
    "# Data preparation, to be run on only one process.\n",
    "sb.utils.distributed.run_on_main(\n",
    "    prepare_user_data,\n",
    "    kwargs={\n",
    "        \"data_folder\": hparams[\"data_folder\"],\n",
    "        \"save_json_train\": hparams[\"train_annotation\"],\n",
    "        \"save_json_valid\": hparams[\"valid_annotation\"],\n",
    "        \"save_json_test\": hparams[\"test_annotation\"],\n",
    "        \"split_ratio\": [80, 10, 10],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Load the pretrained model\n",
    "if \"pretrainer\" in hparams:\n",
    "    hparams[\"pretrainer\"].collect_files()\n",
    "    hparams[\"pretrainer\"].load_collected(device=run_opts[\"device\"])\n",
    "else:\n",
    "    print(\"No pretrained model found, training from scratch.\")\n",
    "    \n",
    "# Create dataset objects \"train\", \"valid\", and \"test\".\n",
    "datasets = dataio_prep(hparams)\n",
    "\n",
    "# Initialize the Brain object to prepare for mask training.\n",
    "spk_id_brain = SpkIdBrain(\n",
    "    modules=hparams[\"modules\"],\n",
    "    opt_class=hparams[\"opt_class\"],\n",
    "    hparams=hparams,\n",
    "    run_opts=run_opts,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    ")   \n",
    "\n",
    "# The `fit()` method iterates the training loop, calling the methods\n",
    "# necessary to update the parameters of the model. Since all objects\n",
    "# with changing state are managed by the Checkpointer, training can be\n",
    "# stopped at any point, and will be resumed on next call.\n",
    "spk_id_brain.fit(\n",
    "    epoch_counter=spk_id_brain.hparams.epoch_counter,\n",
    "    train_set=datasets[\"train\"],\n",
    "    valid_set=datasets[\"valid\"],\n",
    "    train_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
    ")\n",
    "\n",
    "# Load the best checkpoint for evaluation\n",
    "test_stats = spk_id_brain.evaluate(\n",
    "    test_set=datasets[\"test\"],\n",
    "    min_key=\"error\",\n",
    "    test_loader_kwargs=hparams[\"dataloader_options\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification through Inference\n",
    "\n",
    "To verify the identity of an unknown speaker, a test utterance of the unknown speaker is passed as input to the trained DNN. A compact deep embedding associated with the unknown speaker is generated and compared with the compact deep embeddings associated with each of the enrollment speakers through calculation of Cosine Distance Similarity. (Talk about Cosine Distance - include brief background?). The distance between the compared compact deep embeddings corresponds to the likelihood that the unknown speaker belongs to the set of enrolled speakers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for inference\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from random import shuffle\n",
    "from torch.nn import CosineSimilarity \n",
    "from torchaudio import load as load_signal\n",
    "from speechbrain.pretrained import EncoderClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable easy accessibility to the most recently trained model, we first move it to the \"content/best_model\" path along with associated hyperparameters and class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = \"results/speaker_id/1986/save/\"  # Path to trained network checkpoints\n",
    "dest_path = \"content/best_model/\"           # Path to store most recently trained model information \n",
    "\n",
    "if os.path.exists(dest_path):\n",
    "    shutil.rmtree(dest_path)\n",
    "\n",
    "os.mkdir(dest_path)\n",
    "shutil.copy2(\"./hparams_inference.yaml\", dest_path)\n",
    "shutil.copy2(src_path + \"label_encoder.txt\", dest_path)\n",
    "ckpt_files = glob.glob(src_path + \"CKPT*\")\n",
    "if not ckpt_files:\n",
    "    print(\"No trained checkpoints\")\n",
    "    exit(1)\n",
    "latest_ckpt_path = max(ckpt_files, key=os.path.getctime)\n",
    "for file in glob.glob(latest_ckpt_path + \"/*\"):\n",
    "    shutil.copy2(file, dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to begin inference, we identify the path to the recorded test signal and the *unique* user id that the test signal should be tested against. << Briefly explain EncoderClassifier class. >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Classifier\n",
    "classifier = EncoderClassifier.from_hparams(source=\"content/best_model\",  hparams_file='hparams_inference.yaml', savedir=\"content/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/cos_sim.png\" style=\"width: 500px;\"/></center>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Calculation of Cosine Similarity</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity\n",
    "similarity = CosineSimilarity(dim=-1, eps=1e-8) # dim=-1 refers to the last dimension (i.e. the embedding dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The verification process is divided into two sections: extracting vector embeddings for each voice signal and calculating its similarity to one of the recorded samples from the enrolled speaker. To allow for a better measure of speaker validation, we test the test signal against 5 randomly selected voice samples from the enrolled speaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_embeddings(model, wav_audio_file_path: str) -> tuple:\n",
    "    \"\"\"Feature extractor that embeds audio into a vector.\"\"\"\n",
    "    signal, _ = load_signal(wav_audio_file_path)  # Reformat audio signal into a tensor\n",
    "    embeddings = model.encode_batch(\n",
    "        signal\n",
    "    )  # Pass tensor through pretrained neural net and extract representation\n",
    "    return embeddings\n",
    "\n",
    "def verify(s1, s2):\n",
    "    global similarity\n",
    "    THRESHOLD = 0.25\n",
    "    score = similarity(s1, s2) # resulting tensor has scores = embedding dimensionality \n",
    "    for s in score: \n",
    "        if s > THRESHOLD: return True\n",
    "    return False\n",
    "\n",
    "test_emb = extract_audio_embeddings(classifier, test_signal_path)\n",
    "\n",
    "spk_samples = glob.glob(f\"data/user_data/raw/{spk_id}/*/*.wav\")\n",
    "shuffle(spk_samples)\n",
    "for sample_path in spk_samples[:5]: # test on up to 5 random samples\n",
    "    print(f\"Testing sample against {sample_path}\")\n",
    "    sample_emb = extract_audio_embeddings(classifier, sample_path)\n",
    "    if verify(test_emb, sample_emb):\n",
    "        print(\"User Verified\")\n",
    "        exit(0)\n",
    "\n",
    "print(\"Suspicious User - Access Denied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Features to Discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Network Architecture (ECAPA-TDNN architecture) (See [here](https://arxiv.org/pdf/2005.07143.pdf))\n",
    "    - Channel- and context-dependent attention mechanism\n",
    "    - Multi-layer Feature Aggregation (MFA)\n",
    "    - AAMsoftmax loss\n",
    "- Connectionist temporal classification loss (CTC loss)\n",
    "- VAD\n",
    "- Statistical pooling\n",
    "- Data Augmentation (Adding time/frequency dropouts, speed change, environmental corruption, noise addition)\n",
    "- Dropout\n",
    "- Normalisation\n",
    "- Linear Learning Rate Decay and Adam Optimiser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "##### Datasets\n",
    "\n",
    "@InProceedings{Nagrani17,\n",
    "  author       = \"Nagrani, A. and Chung, J.~S. and Zisserman, A.\",\n",
    "  title        = \"VoxCeleb: a large-scale speaker identification dataset\",\n",
    "  booktitle    = \"INTERSPEECH\",\n",
    "  year         = \"2017\",\n",
    "}\n",
    "\n",
    "@InProceedings{Nagrani17,\n",
    "  author       = \"Chung, J.~S. and Nagrani, A. and Zisserman, A.\",\n",
    "  title        = \"VoxCeleb2: Deep Speaker Recognition\",\n",
    "  booktitle    = \"INTERSPEECH\",\n",
    "  year         = \"2018\",\n",
    "}\n",
    "\n",
    "##### Other\n",
    "\n",
    "@misc{speechbrain,\n",
    "  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n",
    "  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n",
    "  year={2021},\n",
    "  eprint={2106.04624},\n",
    "  archivePrefix={arXiv},\n",
    "  primaryClass={eess.AS},\n",
    "  note={arXiv:2106.04624}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bb23381bfbc91ec6d34d7156553c9a17aad40b7e47163f9e724cf2addcd827a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
