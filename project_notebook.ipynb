{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Speaker Verification - A Study in Deep Neural Networks and Transfer Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"Identity theft is not a joke\" - Dwight Shrute, The Office*\n",
    "\n",
    "As the world swiftly shifts towards a technological landscape the need to protect our online identity becomes imperative. Engineers have tackled this challenge through facial and fingerprint detection, although the ability to authenticate a claimed identity through analysing a spoken sample of their voice will completely transform this space. Further, Considering the major advancements of virtual reality, the ability to verify one's voice and in extension recognise their speech will be mainstream in any virtual environments. Our team thus sought to extend our understanding of neural style transfer by exploring how an xvector model could be used for speaker verification.\n",
    "\n",
    "A future experiment of ours is to use a similar framework for speech recognition, and eventually using neural networks to harness the ability of speaking in the voice of another person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "\n",
    "To activate a virtual environment, run\n",
    "\n",
    "```shell\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "To install the required python packages, run\n",
    "\n",
    "```shell\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources\n",
    "\n",
    "https://arxiv.org/pdf/2005.07143.pdf\n",
    "\n",
    "https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio\n",
    "\n",
    "https://becominghuman.ai/convoice-real-time-zero-shot-voice-style-transfer-with-convolutional-network-4c7b7fff66c9\n",
    "\n",
    "https://arxiv.org/pdf/1703.10135.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1905.05879v2.pdf\n",
    "\n",
    "https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer Learning\n",
    "\n",
    "The DNN is trained to classify speakers using a training set of speech recorded from a large number of training speakers. (Talk about Vox-Celeb dataset). To leverage feature representations from the pretrained model on the large dataset, speech recorded from each set of enrollment speakers is passed as input to the trained DNN. This enables the computations of deeper hidden features for each speaker in the enrollment set, which are then averaged to generate a compact deep embedding associated with that speaker.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-Shot Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "Our pretrained model is trained on audio files collected from the VoxCeleb1 + VoxCeleb2 dataset, consisting of speech samples from over 7000 different speakers of a wide range of ethnicities, accents, professions and ages ([SOURCE](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/)). Achieves an accuracy of approximately 98-99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification through Inference\n",
    "\n",
    "To verify the identity of an unknown speaker, a test utterance of the unknown speaker is passed as input to the trained DNN. A compact deep embedding associated with the unknown speaker is generated and compared with the compact deep embeddings associated with each of the enrollment speakers through calculation of Cosine Distance Similarity. (Talk about Cosine Distance - include brief background?). The distance between the compared compact deep embeddings corresponds to the likelihood that the unknown speaker belongs to the set of enrolled speakers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Features to Discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Network Architecture (ECAPA-TDNN architecture) (See [here](https://arxiv.org/pdf/2005.07143.pdf)) (Ahmet)\n",
    "    - Channel- and context-dependent attention mechanism\n",
    "    - Multi-layer Feature Aggregation (MFA)\n",
    "    - AAMsoftmax loss\n",
    "- Connectionist temporal classification loss (CTC loss)\n",
    "- VAD\n",
    "- Statistical pooling (Ahmet)\n",
    "- Data Augmentation (Adding time/frequency dropouts, speed change, environmental corruption, noise addition) (Armaan)\n",
    "- Dropout\n",
    "- Normalisation\n",
    "- Linear Learning Rate Decay and Adam Optimiser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion ###\n",
    "While speaker verification on its own cannot guarantee security, it will add strength and friction to our online identities and reduce the likelihood of incorrect authentication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citation\n",
    "\n",
    "##### Datasets\n",
    "\n",
    "@InProceedings{Nagrani17,\n",
    "  author       = \"Nagrani, A. and Chung, J.~S. and Zisserman, A.\",\n",
    "  title        = \"VoxCeleb: a large-scale speaker identification dataset\",\n",
    "  booktitle    = \"INTERSPEECH\",\n",
    "  year         = \"2017\",\n",
    "}\n",
    "\n",
    "@InProceedings{Nagrani17,\n",
    "  author       = \"Chung, J.~S. and Nagrani, A. and Zisserman, A.\",\n",
    "  title        = \"VoxCeleb2: Deep Speaker Recognition\",\n",
    "  booktitle    = \"INTERSPEECH\",\n",
    "  year         = \"2018\",\n",
    "}\n",
    "\n",
    "##### Other\n",
    "\n",
    "@misc{speechbrain,\n",
    "  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n",
    "  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n",
    "  year={2021},\n",
    "  eprint={2106.04624},\n",
    "  archivePrefix={arXiv},\n",
    "  primaryClass={eess.AS},\n",
    "  note={arXiv:2106.04624}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
