{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Speaker Verification - A Study in Deep Neural Networks and Transfer Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"Identity theft is not a joke\" - Dwight Shrute, The Office*\n",
    "\n",
    "Automated speaker verification (ASV), in the modern day, is omnipresent in smart devices and in services offered by call centres. It serves as a biometric means of authenticating a claimed identity by analysing a spoken sample of their voice. \n",
    "our team sought to extend our understanding of neural style transfer by exploring how convolutional neural networks could be used to harness the ability to speak in the voice of another person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "\n",
    "To activate a virtual environment, run\n",
    "\n",
    "```shell\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "To install the required python packages, run\n",
    "\n",
    "```shell\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources\n",
    "\n",
    "https://arxiv.org/pdf/2005.07143.pdf\n",
    "\n",
    "https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio\n",
    "\n",
    "https://becominghuman.ai/convoice-real-time-zero-shot-voice-style-transfer-with-convolutional-network-4c7b7fff66c9\n",
    "\n",
    "https://arxiv.org/pdf/1703.10135.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1905.05879v2.pdf\n",
    "\n",
    "https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer Learning\n",
    "\n",
    "The DNN is trained to classify speakers using a training set of speech recorded from a large number of training speakers. (Talk about Vox-Celeb dataset). To leverage feature representations from the pretrained model on the large dataset, speech recorded from each set of enrollment speakers is passed as input to the trained DNN. This enables the computations of deeper hidden features for each speaker in the enrollment set, which are then averaged to generate a compact deep embedding associated with that speaker.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-Shot Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "Our pretrained model is trained on audio files collected from the VoxCeleb1 + VoxCeleb2 dataset, consisting of speech samples from over 7000 different speakers of a wide range of ethnicities, accents, professions and ages ([SOURCE](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/)). Achieves an accuracy of approximately 98-99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification through Inference\n",
    "\n",
    "To verify the identity of an unknown speaker, a test utterance of the unknown speaker is passed as input to the trained DNN. A compact deep embedding associated with the unknown speaker is generated and compared with the compact deep embeddings associated with each of the enrollment speakers through calculation of Cosine Distance Similarity. (Talk about Cosine Distance - include brief background?). The distance between the compared compact deep embeddings corresponds to the likelihood that the unknown speaker belongs to the set of enrolled speakers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for inference\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from random import shuffle\n",
    "from torch.nn import CosineSimilarity \n",
    "from torchaudio import load as load_signal\n",
    "from speechbrain.pretrained import EncoderClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable easy accessibility to the most recently trained model, we first move it to the \"content/best_model\" path along with associated hyperparameters and class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = \"results/speaker_id/1986/save/\"  # Path to trained network checkpoints\n",
    "dest_path = \"content/best_model/\"           # Path to store most recently trained model information \n",
    "\n",
    "if os.path.exists(dest_path):\n",
    "    shutil.rmtree(dest_path)\n",
    "\n",
    "os.mkdir(dest_path)\n",
    "shutil.copy2(\"./hparams_inference.yaml\", dest_path)\n",
    "shutil.copy2(src_path + \"label_encoder.txt\", dest_path)\n",
    "ckpt_files = glob.glob(src_path + \"CKPT*\")\n",
    "if not ckpt_files:\n",
    "    print(\"No trained checkpoints\")\n",
    "    exit(1)\n",
    "latest_ckpt_path = max(ckpt_files, key=os.path.getctime)\n",
    "for file in glob.glob(latest_ckpt_path + \"/*\"):\n",
    "    shutil.copy2(file, dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to begin inference, we identify the path to the recorded test signal and the *unique* user id that the test signal should be tested against. << Briefly explain EncoderClassifier class. >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Classifier\n",
    "classifier = EncoderClassifier.from_hparams(source=\"content/best_model\",  hparams_file='hparams_inference.yaml', savedir=\"content/best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity\n",
    "similarity = CosineSimilarity(dim=-1, eps=1e-8) # dim=-1 refers to the last dimension (i.e. the embedding dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Calculation of Cosine Similarity](\"other/cos_sim.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_embeddings(model, wav_audio_file_path: str) -> tuple:\n",
    "    \"\"\"Feature extractor that embeds audio into a vector.\"\"\"\n",
    "    signal, _ = load_signal(wav_audio_file_path)  # Reformat audio signal into a tensor\n",
    "    embeddings = model.encode_batch(\n",
    "        signal\n",
    "    )  # Pass tensor through pretrained neural net and extract representation\n",
    "    return embeddings\n",
    "\n",
    "def verify(s1, s2):\n",
    "    global similarity\n",
    "    score = similarity(s1, s2) # resulting tensor has scores = embedding dimensionality \n",
    "    for s in score: \n",
    "        if s > 0.25: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb = extract_audio_embeddings(classifier, test_signal_path)\n",
    "\n",
    "spk_samples = glob.glob(f\"data/user_data/raw/{spk_id}/*/*.wav\")\n",
    "shuffle(spk_samples)\n",
    "for sample_path in spk_samples[:5]: # test on up to 5 random samples\n",
    "    print(f\"Testing sample against {sample_path}\")\n",
    "    sample_emb = extract_audio_embeddings(classifier, sample_path)\n",
    "    if verify(test_emb, sample_emb):\n",
    "        print(\"User Verified\")\n",
    "        exit(0)\n",
    "\n",
    "print(\"Suspicious User - Access Denied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Features to Discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Network Architecture (ECAPA-TDNN architecture) (See [here](https://arxiv.org/pdf/2005.07143.pdf))\n",
    "    - Channel- and context-dependent attention mechanism\n",
    "    - Multi-layer Feature Aggregation (MFA)\n",
    "    - AAMsoftmax loss\n",
    "- Connectionist temporal classification loss (CTC loss)\n",
    "- VAD\n",
    "- Statistical pooling\n",
    "- Data Augmentation (Adding time/frequency dropouts, speed change, environmental corruption, noise addition)\n",
    "- Dropout\n",
    "- Normalisation\n",
    "- Linear Learning Rate Decay and Adam Optimiser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citation\n",
    "\n",
    "##### Datasets\n",
    "\n",
    "@InProceedings{Nagrani17,\n",
    "  author       = \"Nagrani, A. and Chung, J.~S. and Zisserman, A.\",\n",
    "  title        = \"VoxCeleb: a large-scale speaker identification dataset\",\n",
    "  booktitle    = \"INTERSPEECH\",\n",
    "  year         = \"2017\",\n",
    "}\n",
    "\n",
    "@InProceedings{Nagrani17,\n",
    "  author       = \"Chung, J.~S. and Nagrani, A. and Zisserman, A.\",\n",
    "  title        = \"VoxCeleb2: Deep Speaker Recognition\",\n",
    "  booktitle    = \"INTERSPEECH\",\n",
    "  year         = \"2018\",\n",
    "}\n",
    "\n",
    "##### Other\n",
    "\n",
    "@misc{speechbrain,\n",
    "  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n",
    "  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n",
    "  year={2021},\n",
    "  eprint={2106.04624},\n",
    "  archivePrefix={arXiv},\n",
    "  primaryClass={eess.AS},\n",
    "  note={arXiv:2106.04624}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bb23381bfbc91ec6d34d7156553c9a17aad40b7e47163f9e724cf2addcd827a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
